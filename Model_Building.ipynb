{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "39d0725753d0ad0b2b78de10b3b7451c8aa9b3c904c7c1796a98e42af75eb3a5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Predicting if Someone has Tried Cocaine\n",
    "## Model Building"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Before we can use our data to train models, we need to do a few things:\n",
    "\n",
    "1. Select our desired features\n",
    "\n",
    "2. Apply standard scaling to our numerical variables\n",
    "\n",
    "3. Dummify/One Hot Encode our categorical variables\n",
    "\n",
    "We begin with removing coutyp4, year, irwrkstat, and mjever columns. These columns were only used for EDA or do not seem to provide insight in target prediction."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        cig30use  iralcfy  irmjfy  irherfy  irmethamyfq  health  irsex  \\\n",
       "0            0.0        5       0        0            0     2.0      1   \n",
       "1            0.0       52     364        0            0     1.0      1   \n",
       "2            0.0       48       0        0            0     2.0      0   \n",
       "4           22.0        6       0        0            0     3.0      0   \n",
       "5            0.0      120       0        0            0     3.0      1   \n",
       "...          ...      ...     ...      ...          ...     ...    ...   \n",
       "282760       0.0        0       0        0            0     1.0      1   \n",
       "282762       0.0        0       0        0            0     3.0      1   \n",
       "282763       0.0      104       2        0            0     2.0      0   \n",
       "282764       0.0       10       0        0            0     2.0      0   \n",
       "282766       0.0        1       0        0            0     2.0      0   \n",
       "\n",
       "        ireduhighst2  catag3  newrace2  wrkdhrswk2  irhhsiz2  irki17_2  \\\n",
       "0                  7       1         1         0.0         1         2   \n",
       "1                  8       4         7        40.0         4         3   \n",
       "2                 11       3         1         0.0         1         1   \n",
       "4                  9       2         1         0.0         4         3   \n",
       "5                  9       2         5         0.0         2         1   \n",
       "...              ...     ...       ...         ...       ...       ...   \n",
       "282760             8       4         1         0.0         3         1   \n",
       "282762             1       4         7         0.0         3         1   \n",
       "282763             9       2         7        40.0         2         1   \n",
       "282764            11       3         5        26.0         2         1   \n",
       "282766            11       1         7         0.0         6         4   \n",
       "\n",
       "        irpinc3  coccrkever  \n",
       "0             1         0.0  \n",
       "1             2         1.0  \n",
       "2             1         0.0  \n",
       "4             1         0.0  \n",
       "5             1         0.0  \n",
       "...         ...         ...  \n",
       "282760        1         0.0  \n",
       "282762        3         0.0  \n",
       "282763        3         0.0  \n",
       "282764        2         0.0  \n",
       "282766        1         0.0  \n",
       "\n",
       "[226977 rows x 15 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cig30use</th>\n      <th>iralcfy</th>\n      <th>irmjfy</th>\n      <th>irherfy</th>\n      <th>irmethamyfq</th>\n      <th>health</th>\n      <th>irsex</th>\n      <th>ireduhighst2</th>\n      <th>catag3</th>\n      <th>newrace2</th>\n      <th>wrkdhrswk2</th>\n      <th>irhhsiz2</th>\n      <th>irki17_2</th>\n      <th>irpinc3</th>\n      <th>coccrkever</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>7</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>52</td>\n      <td>364</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>8</td>\n      <td>4</td>\n      <td>7</td>\n      <td>40.0</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>48</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>11</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>22.0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.0</td>\n      <td>120</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>9</td>\n      <td>2</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>282760</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>8</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>282762</th>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>7</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>282763</th>\n      <td>0.0</td>\n      <td>104</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2</td>\n      <td>7</td>\n      <td>40.0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>282764</th>\n      <td>0.0</td>\n      <td>10</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>11</td>\n      <td>3</td>\n      <td>5</td>\n      <td>26.0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>282766</th>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>11</td>\n      <td>1</td>\n      <td>7</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>226977 rows × 15 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df = pd.read_pickle(\"./pickle/NSDUH_cleaned_dropna_2016-2019.pkl\")\n",
    "df = df.drop(['cocever', 'crkever', 'year'], axis=1)\n",
    "df"
   ]
  },
  {
   "source": [
    "Now, we must separate our numerical and categorical variables. Numerical variables will be adjusted per column by StandardScaler(), which converts the data such that the mean and standard deviation is 0 and 1 for that column, respectively. This standardization across numerical variables increases our model's accuracy.\n",
    "\n",
    "As for categorical variables, each unique value in a categorical column must be given its own separate, binary column indicating if that observation fits that unique value or not. We do this because keeping them in one column implies some kind of order. Something like race (newrace2 column) makes no sense to order, and is therefore a candidate to be separated into different columns (dummified). "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous variables\n",
    "num_cols = [\n",
    "    \"iralcfy\",\n",
    "    \"catag3\",\n",
    "    \"health\",\n",
    "    \"ireduhighst2\",\n",
    "    \"irpinc3\",\n",
    "    \"irki17_2\",\n",
    "    \"irmjfy\",\n",
    "    \"wrkdhrswk2\",\n",
    "    'irhhsiz2',\n",
    "    'cig30use',\n",
    "    'irherfy',\n",
    "    'irmethamyfq'\n",
    "]\n",
    "\n",
    "# Categorical variables, excluding irsex, which is already properly formatted\n",
    "cat_cols = [\n",
    "    \"newrace2\",\n",
    "    \"irsex\"\n",
    "]"
   ]
  },
  {
   "source": [
    "# Data Preprocessing with Pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(drop='first'), cat_cols)\n",
    "])"
   ]
  },
  {
   "source": [
    "# Model Building\n",
    "\n",
    "Now that our data is properly processed, we can test several models across different hyperparameters using GridSearchCV. We will be testing the following models:\n",
    "\n",
    "1. RandomForestClassifier()\n",
    "\n",
    "2. LogisticRegression()\n",
    "\n",
    "3. svm.LinearSVC()\n",
    "\n",
    "*We will only test the default hyperparamters for our LinearSVC due to the length of time required to train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature and target columns\n",
    "features = num_cols+cat_cols\n",
    "target = \"coccrkever\"\n",
    "\n",
    "# Standard naming conventions for feature/test datasets\n",
    "X = df[features]\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(drop='first'), cat_cols)\n",
    "])\n",
    "\n",
    "# Parameter grid for GridSearchCV\n",
    "model_grid = {\n",
    "    'random_forest': {\n",
    "        'model':RandomForestClassifier(random_state=15, n_jobs=3, n_estimators=500),\n",
    "        'params': {\n",
    "            'estimator__max_depth': [11, 12, 13, 14],\n",
    "            'estimator__criterion':['gini', 'entropy']\n",
    "        }\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'model': LogisticRegression(random_state=15, n_jobs=3),\n",
    "        'params': {\n",
    "            'estimator__C': [0.085, 0.09, 0.092],\n",
    "            'estimator__solver':['lbfgs', 'liblinear'],\n",
    "        }\n",
    "    },\n",
    "    'svm': {\n",
    "        'model': svm.LinearSVC(random_state=15, max_iter=100000),\n",
    "        'params': {\n",
    "            'estimator__C':[0.52, 0.55, 0.6, 0.65]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Brennan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\Brennan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\Brennan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\Brennan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\Brennan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\Brennan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\Brennan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\Brennan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\Brennan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\Brennan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\Brennan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\Brennan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\Brennan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n"
     ]
    }
   ],
   "source": [
    "# List to hold model scores\n",
    "scores = []\n",
    "\n",
    "for model_name, model_params in model_grid.items():\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('estimator', model_params['model'])\n",
    "    ])\n",
    "\n",
    "    model = GridSearchCV(estimator=pipe, param_grid=model_params['params'], cv=4, return_train_score=False, refit=True)\n",
    "    model.fit(X, y)\n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score:': model.best_score_,\n",
    "        'best_params': model.best_params_\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'model': 'random_forest',\n",
       "  'best_score:': 0.8858342496707732,\n",
       "  'best_params': {'estimator__criterion': 'entropy',\n",
       "   'estimator__max_depth': 14}},\n",
       " {'model': 'logistic_regression',\n",
       "  'best_score:': 0.8781065916890805,\n",
       "  'best_params': {'estimator__C': 0.085, 'estimator__solver': 'liblinear'}},\n",
       " {'model': 'svm',\n",
       "  'best_score:': 0.877463356611295,\n",
       "  'best_params': {'estimator__C': 0.52}}]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Show scores as df\n",
    "scores_df = pd.DataFrame(scores)\n",
    "scores"
   ]
  },
  {
   "source": [
    "# Choosing a Model\n",
    "\n",
    "Although our random forest model has the highest accuracy, the accuracies are very similar. Let's train a model using the best hyperparameters of each, then look at the classification report of each model to gain better insight into the performance of each model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=.25, random_state=12)\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(drop='first'), cat_cols)\n",
    "])\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "    ])\n",
    "\n",
    "X_train = pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LinearSVC(C=0.52, max_iter=100000, random_state=15)"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# Train a model for each model type using our best hyperparameters. This is so we can analyze each\n",
    "# in a classification report\n",
    "\n",
    "rf = RandomForestClassifier(random_state=15, n_jobs=3, n_estimators=500, max_depth=14, criterion='entropy')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "lg = LogisticRegression(random_state=15, solver='liblinear',C=0.085)\n",
    "lg.fit(X_train, y_train)\n",
    "\n",
    "lsvc = svm.LinearSVC(random_state=15, max_iter=100000, C=0.52)\n",
    "lsvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pipe.transform(X_test)\n",
    "\n",
    "rf_predict = rf.predict(X_test)\n",
    "lg_predict = lg.predict(X_test)\n",
    "lsvc_predict = lsvc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        iralcfy    catag3    health  ireduhighst2   irpinc3  irki17_2  \\\n",
       "0     -0.661241 -1.516697 -1.287575     -1.658407 -0.913660  1.001951   \n",
       "1     -0.042358  0.725999 -0.262482      1.209393  1.500373  1.001951   \n",
       "2      0.576525  1.473564  2.812798      1.209393 -0.430854 -0.874318   \n",
       "3     -0.042358  0.725999  1.787705      0.253460 -0.430854  1.940085   \n",
       "4     -0.232784 -0.769131  0.762612     -0.224507 -0.913660 -0.874318   \n",
       "...         ...       ...       ...           ...       ...       ...   \n",
       "56740 -0.089965 -0.021566  1.787705      1.209393  1.500373 -0.874318   \n",
       "56741  3.052056 -0.769131 -1.287575      0.253460 -0.913660 -0.874318   \n",
       "56742 -0.661241  1.473564 -1.287575     -0.224507 -0.913660 -0.874318   \n",
       "56743 -0.232784 -0.769131  0.762612      1.209393  0.534760 -0.874318   \n",
       "56744 -0.661241 -0.021566 -0.262482     -0.224507 -0.913660 -0.874318   \n",
       "\n",
       "         irmjfy  wrkdhrswk2  irhhsiz2  cig30use   irherfy  irmethamyfq  \\\n",
       "0     -0.333690   -1.034050  0.491784 -0.428314 -0.046153    -0.060157   \n",
       "1     -0.321278    1.176180  0.491784 -0.428314 -0.046153    -0.060157   \n",
       "2     -0.333690   -1.034050 -0.908570 -0.428314 -0.046153    -0.060157   \n",
       "3      3.948377   -1.034050  0.491784 -0.428314 -0.046153    -0.060157   \n",
       "4     -0.333690   -0.641120 -0.208393 -0.428314 -0.046153    -0.060157   \n",
       "...         ...         ...       ...       ...       ...          ...   \n",
       "56740 -0.333690    0.930599 -1.608748 -0.428314 -0.046153    -0.060157   \n",
       "56741 -0.184748   -0.739353 -0.208393 -0.428314 -0.046153    -0.060157   \n",
       "56742 -0.333690   -1.034050 -0.908570 -0.428314 -0.046153    -0.060157   \n",
       "56743 -0.333690    0.930599 -1.608748 -0.125412 -0.046153    -0.060157   \n",
       "56744 -0.333690   -1.034050 -0.908570  2.600706 -0.046153    -0.060157   \n",
       "\n",
       "       newrace2_2  newrace2_3  newrace2_4  newrace2_5  newrace2_6  newrace2_7  \\\n",
       "0             0.0         0.0         0.0         0.0         0.0         1.0   \n",
       "1             0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2             0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "3             1.0         0.0         0.0         0.0         0.0         0.0   \n",
       "4             0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "56740         0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "56741         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "56742         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "56743         1.0         0.0         0.0         0.0         0.0         0.0   \n",
       "56744         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "       irsex  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        1.0  \n",
       "...      ...  \n",
       "56740    0.0  \n",
       "56741    0.0  \n",
       "56742    0.0  \n",
       "56743    1.0  \n",
       "56744    0.0  \n",
       "\n",
       "[56745 rows x 19 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>iralcfy</th>\n      <th>catag3</th>\n      <th>health</th>\n      <th>ireduhighst2</th>\n      <th>irpinc3</th>\n      <th>irki17_2</th>\n      <th>irmjfy</th>\n      <th>wrkdhrswk2</th>\n      <th>irhhsiz2</th>\n      <th>cig30use</th>\n      <th>irherfy</th>\n      <th>irmethamyfq</th>\n      <th>newrace2_2</th>\n      <th>newrace2_3</th>\n      <th>newrace2_4</th>\n      <th>newrace2_5</th>\n      <th>newrace2_6</th>\n      <th>newrace2_7</th>\n      <th>irsex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.661241</td>\n      <td>-1.516697</td>\n      <td>-1.287575</td>\n      <td>-1.658407</td>\n      <td>-0.913660</td>\n      <td>1.001951</td>\n      <td>-0.333690</td>\n      <td>-1.034050</td>\n      <td>0.491784</td>\n      <td>-0.428314</td>\n      <td>-0.046153</td>\n      <td>-0.060157</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.042358</td>\n      <td>0.725999</td>\n      <td>-0.262482</td>\n      <td>1.209393</td>\n      <td>1.500373</td>\n      <td>1.001951</td>\n      <td>-0.321278</td>\n      <td>1.176180</td>\n      <td>0.491784</td>\n      <td>-0.428314</td>\n      <td>-0.046153</td>\n      <td>-0.060157</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.576525</td>\n      <td>1.473564</td>\n      <td>2.812798</td>\n      <td>1.209393</td>\n      <td>-0.430854</td>\n      <td>-0.874318</td>\n      <td>-0.333690</td>\n      <td>-1.034050</td>\n      <td>-0.908570</td>\n      <td>-0.428314</td>\n      <td>-0.046153</td>\n      <td>-0.060157</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.042358</td>\n      <td>0.725999</td>\n      <td>1.787705</td>\n      <td>0.253460</td>\n      <td>-0.430854</td>\n      <td>1.940085</td>\n      <td>3.948377</td>\n      <td>-1.034050</td>\n      <td>0.491784</td>\n      <td>-0.428314</td>\n      <td>-0.046153</td>\n      <td>-0.060157</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.232784</td>\n      <td>-0.769131</td>\n      <td>0.762612</td>\n      <td>-0.224507</td>\n      <td>-0.913660</td>\n      <td>-0.874318</td>\n      <td>-0.333690</td>\n      <td>-0.641120</td>\n      <td>-0.208393</td>\n      <td>-0.428314</td>\n      <td>-0.046153</td>\n      <td>-0.060157</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>56740</th>\n      <td>-0.089965</td>\n      <td>-0.021566</td>\n      <td>1.787705</td>\n      <td>1.209393</td>\n      <td>1.500373</td>\n      <td>-0.874318</td>\n      <td>-0.333690</td>\n      <td>0.930599</td>\n      <td>-1.608748</td>\n      <td>-0.428314</td>\n      <td>-0.046153</td>\n      <td>-0.060157</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>56741</th>\n      <td>3.052056</td>\n      <td>-0.769131</td>\n      <td>-1.287575</td>\n      <td>0.253460</td>\n      <td>-0.913660</td>\n      <td>-0.874318</td>\n      <td>-0.184748</td>\n      <td>-0.739353</td>\n      <td>-0.208393</td>\n      <td>-0.428314</td>\n      <td>-0.046153</td>\n      <td>-0.060157</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>56742</th>\n      <td>-0.661241</td>\n      <td>1.473564</td>\n      <td>-1.287575</td>\n      <td>-0.224507</td>\n      <td>-0.913660</td>\n      <td>-0.874318</td>\n      <td>-0.333690</td>\n      <td>-1.034050</td>\n      <td>-0.908570</td>\n      <td>-0.428314</td>\n      <td>-0.046153</td>\n      <td>-0.060157</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>56743</th>\n      <td>-0.232784</td>\n      <td>-0.769131</td>\n      <td>0.762612</td>\n      <td>1.209393</td>\n      <td>0.534760</td>\n      <td>-0.874318</td>\n      <td>-0.333690</td>\n      <td>0.930599</td>\n      <td>-1.608748</td>\n      <td>-0.125412</td>\n      <td>-0.046153</td>\n      <td>-0.060157</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>56744</th>\n      <td>-0.661241</td>\n      <td>-0.021566</td>\n      <td>-0.262482</td>\n      <td>-0.224507</td>\n      <td>-0.913660</td>\n      <td>-0.874318</td>\n      <td>-0.333690</td>\n      <td>-1.034050</td>\n      <td>-0.908570</td>\n      <td>2.600706</td>\n      <td>-0.046153</td>\n      <td>-0.060157</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>56745 rows × 19 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "output_cols = [\n",
    "    \"iralcfy\",\n",
    "    \"catag3\",\n",
    "    \"health\",\n",
    "    \"ireduhighst2\",\n",
    "    \"irpinc3\",\n",
    "    \"irki17_2\",\n",
    "    \"irmjfy\",\n",
    "    \"wrkdhrswk2\",\n",
    "    'irhhsiz2',\n",
    "    'cig30use',\n",
    "    'irherfy',\n",
    "    'irmethamyfq',\n",
    "    'newrace2_2',\n",
    "    'newrace2_3',\n",
    "    'newrace2_4',\n",
    "    'newrace2_5',\n",
    "    'newrace2_6',\n",
    "    'newrace2_7',\n",
    "    \"irsex\"\n",
    "]\n",
    "\n",
    "X_test = pd.DataFrame(X_test, columns=output_cols)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random Forest Score: 0.885276\n",
      "Logistic Regression Score: 0.877187\n",
      "Linear SVC Score: 0.876764\n",
      "\n",
      "Random Forest:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.94     49154\n",
      "         1.0       0.68      0.27      0.39      7591\n",
      "\n",
      "    accuracy                           0.89     56745\n",
      "   macro avg       0.79      0.63      0.66     56745\n",
      "weighted avg       0.87      0.89      0.86     56745\n",
      "\n",
      "Logistic Regression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.98      0.93     49154\n",
      "         1.0       0.63      0.20      0.31      7591\n",
      "\n",
      "    accuracy                           0.88     56745\n",
      "   macro avg       0.76      0.59      0.62     56745\n",
      "weighted avg       0.85      0.88      0.85     56745\n",
      "\n",
      "Linear SVC:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.99      0.93     49154\n",
      "         1.0       0.66      0.17      0.26      7591\n",
      "\n",
      "    accuracy                           0.88     56745\n",
      "   macro avg       0.77      0.58      0.60     56745\n",
      "weighted avg       0.85      0.88      0.84     56745\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest Score: %f\\nLogistic Regression Score: %f\\nLinear SVC Score: %f\\n\" %(accuracy_score(y_test, rf_predict), accuracy_score(y_test, lg_predict), accuracy_score(y_test, lsvc_predict)))\n",
    "print(\"Random Forest:\\n\", classification_report(y_test, rf_predict))\n",
    "print(\"Logistic Regression:\\n\", classification_report(y_test, lg_predict))\n",
    "print(\"Linear SVC:\\n\", classification_report(y_test, lsvc_predict))"
   ]
  },
  {
   "source": [
    "## Accuracy vs Precision vs Recall\n",
    "\n",
    "Although the random forest performed the best in terms of total accuracy, our linear SVC model has the highest precision of each model. Recall teh differences between accuracy, precision, and recall:\n",
    "\n",
    "1. **Accuracy**: Proportion of correct predictions from total observations\n",
    "\n",
    "2. **Precision**: For a given class, the proportion of correct predictions from total predictions\n",
    "\n",
    "3. **Recall**: For a given class, proportion of correct predictions from the total number of true observations for that class\n",
    "\n",
    "Our models have low recall. That means we miss a large number of people who have actually used cocaine. However, we also have extremely high precision. This means that for the people we *do* predict have used cocaine, we are actually correct! This is important to consider. If your goal is to either help people using cocaine or prevent people from becoming addicted cocaine, it would be very bad to wrongly approach someone believing they've tried cocaine when they actually have not. **To prevent false positives, we will choose our Linear SVC model because of its extremely high precision.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Saving our Data\n",
    "Although we've decided on using the linear SVC model, we will save all the models regardless, just in case we want them in the future."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle models for later\n",
    "for model, name in zip([lg, lsvc], [\"logreg_model\", \"lsvc_model\"]):\n",
    "    with open(\"model/\" + name + \".pickle\", 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Code for loading from a gzipped pickle file'"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "import gzip, pickletools\n",
    "\n",
    "# The output of a regular pickle.dump for our random forest is quite large,\n",
    "# we can compress it using gzip\n",
    "with gzip.open(\"model/randforest_model.pickle\", \"wb\") as f:\n",
    "    pickled = pickle.dumps(rf)\n",
    "    optimized_pickle = pickletools.optimize(pickled)\n",
    "    f.write(optimized_pickle)\n",
    "\n",
    "\"\"\"Code for loading from a gzipped pickle file\"\"\"\n",
    "# with gzip.open(\"model/randforest_model_optimized.pickle\", 'rb') as f:\n",
    "#     p = pickle.Unpickler(f)\n",
    "#     rf = p.load()"
   ]
  },
  {
   "source": [
    "Next, we need to save our fitted pipeline to transform future data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle models for later\n",
    "with open(\"model/pipeline.pickle\", 'wb') as f:\n",
    "        pickle.dump(pipe, f)"
   ]
  },
  {
   "source": [
    "Finally, let's save our columns as a JSON file for future reference."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "column_info = {\n",
    "    'data_columns' : [col for col in num_cols+cat_cols]\n",
    "}\n",
    "col_desc = pd.read_csv('model/col_desc.txt', header=0, sep='\\t')\n",
    "for row in range(col_desc.shape[0]):\n",
    "    column_info[col_desc.iloc[row, 0]] = col_desc.iloc[row, 1]\n",
    "\n",
    "with open(\"model/data_columns.json\", \"w\") as f:\n",
    "    f.write(json.dumps(column_info))"
   ]
  },
  {
   "source": [
    "Now, we can move on to creating a server where we can make our model easily interactable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}